GPU Prolog Script v1.14
This is a GPU node.
Enough GPUs available.
Allocating card 3
doglion.local
Thu  8 Aug 11:02:13 BST 2024
Time56h
ic| 'parser and tokeniser'
ic| 'importing embeddings'
ic| 'reading data'
ic| len(values): 52
ic| TESTING: False
ic| 'generating diagrams'
Tagging sentences.
Parsing tagged sentences.
Turning parse trees to diagrams.
Tagging sentences.
Parsing tagged sentences.
Turning parse trees to diagrams.
Tagging sentences.
Parsing tagged sentences.
Turning parse trees to diagrams.
Tagging sentences.
Parsing tagged sentences.
Turning parse trees to diagrams.
Tagging sentences.
Parsing tagged sentences.
Turning parse trees to diagrams.
ic| 'Finished importing embeddings'
ic| ansatz_string: 'FslNN'
ic| 'Turning sentences to circuits'
ic| ansatz_string: 'FslNN'
ic| map: {grammar.pregroup.Ty(rigid.Ob('n')): 3, grammar.pregroup.Ty(rigid.Ob('s')): 1}
ic| 'Circuit Processing finished'
ic| 'Setting model'
ic| 'setting model'
ic| EPOCHS: 1500, SEED: 400, BATCH_SIZE: 700
Epoch 10:    train/loss: 0.6020   valid/loss: 0.6067   train/acc: 0.6271   valid/acc: 0.6935
Epoch 20:    train/loss: 0.6516   valid/loss: 0.6001   train/acc: 0.6829   valid/acc: 0.7011
Epoch 30:    train/loss: 0.6189   valid/loss: 0.5984   train/acc: 0.6756   valid/acc: 0.7011
Epoch 40:    train/loss: 0.5756   valid/loss: 0.5997   train/acc: 0.6921   valid/acc: 0.7011
Epoch 50:    train/loss: 0.5622   valid/loss: 0.5992   train/acc: 0.6934   valid/acc: 0.7011
Epoch 60:    train/loss: 0.5910   valid/loss: 0.5990   train/acc: 0.6901   valid/acc: 0.7011
Epoch 70:    train/loss: 0.6585   valid/loss: 0.5984   train/acc: 0.6921   valid/acc: 0.7011
Epoch 80:    train/loss: 0.5900   valid/loss: 0.5985   train/acc: 0.6894   valid/acc: 0.7011
Epoch 90:    train/loss: 0.6284   valid/loss: 0.5984   train/acc: 0.6376   valid/acc: 0.7011
Epoch 100:   train/loss: 0.6572   valid/loss: 0.5988   train/acc: 0.6881   valid/acc: 0.7011
Epoch 110:   train/loss: 0.6105   valid/loss: 0.5991   train/acc: 0.6921   valid/acc: 0.7011
Epoch 120:   train/loss: 0.5493   valid/loss: 0.5989   train/acc: 0.6888   valid/acc: 0.7011
Epoch 130:   train/loss: 0.5839   valid/loss: 0.5988   train/acc: 0.6888   valid/acc: 0.7011
Epoch 140:   train/loss: 0.5439   valid/loss: 0.5989   train/acc: 0.6921   valid/acc: 0.7011
Epoch 150:   train/loss: 0.5834   valid/loss: 0.5988   train/acc: 0.6921   valid/acc: 0.7011
Epoch 160:   train/loss: 0.5735   valid/loss: 0.5986   train/acc: 0.6921   valid/acc: 0.7011
Epoch 170:   train/loss: 0.6128   valid/loss: 0.5987   train/acc: 0.6921   valid/acc: 0.7011
Epoch 180:   train/loss: 0.6268   valid/loss: 0.5986   train/acc: 0.6921   valid/acc: 0.7011
Epoch 190:   train/loss: 0.6076   valid/loss: 0.5985   train/acc: 0.6875   valid/acc: 0.7011
Epoch 200:   train/loss: 0.6099   valid/loss: 0.5984   train/acc: 0.6875   valid/acc: 0.7011
Epoch 210:   train/loss: 0.5785   valid/loss: 0.5982   train/acc: 0.6921   valid/acc: 0.7011
Epoch 220:   train/loss: 0.5784   valid/loss: 0.5985   train/acc: 0.6921   valid/acc: 0.7011
Epoch 230:   train/loss: 0.6715   valid/loss: 0.5984   train/acc: 0.6921   valid/acc: 0.7011
Epoch 240:   train/loss: 0.5980   valid/loss: 0.5982   train/acc: 0.6901   valid/acc: 0.7011
Epoch 250:   train/loss: 0.6197   valid/loss: 0.5982   train/acc: 0.6921   valid/acc: 0.7011
Epoch 260:   train/loss: 0.6505   valid/loss: 0.5985   train/acc: 0.6921   valid/acc: 0.7011
Epoch 270:   train/loss: 0.6149   valid/loss: 0.5985   train/acc: 0.6901   valid/acc: 0.7011
Epoch 280:   train/loss: 0.6118   valid/loss: 0.5986   train/acc: 0.6921   valid/acc: 0.7011
Epoch 290:   train/loss: 0.6568   valid/loss: 0.5987   train/acc: 0.6921   valid/acc: 0.7011
Epoch 300:   train/loss: 0.6138   valid/loss: 0.5989   train/acc: 0.6921   valid/acc: 0.7011
Epoch 310:   train/loss: 0.6296   valid/loss: 0.5990   train/acc: 0.6921   valid/acc: 0.7011
Epoch 320:   train/loss: 0.5965   valid/loss: 0.5992   train/acc: 0.6921   valid/acc: 0.7011
Epoch 330:   train/loss: 0.6042   valid/loss: 0.5991   train/acc: 0.6921   valid/acc: 0.7011
Epoch 340:   train/loss: 0.5915   valid/loss: 0.5991   train/acc: 0.6921   valid/acc: 0.7011
Epoch 350:   train/loss: 0.6200   valid/loss: 0.5989   train/acc: 0.6921   valid/acc: 0.7011
Epoch 360:   train/loss: 0.6573   valid/loss: 0.5989   train/acc: 0.6921   valid/acc: 0.7011
Epoch 370:   train/loss: 0.6070   valid/loss: 0.5988   train/acc: 0.6921   valid/acc: 0.7011
Epoch 380:   train/loss: 0.6213   valid/loss: 0.5987   train/acc: 0.6921   valid/acc: 0.7011
Epoch 390:   train/loss: 0.5821   valid/loss: 0.5986   train/acc: 0.6921   valid/acc: 0.7011
Epoch 400:   train/loss: 0.5776   valid/loss: 0.5988   train/acc: 0.6921   valid/acc: 0.7011
Epoch 410:   train/loss: 0.6171   valid/loss: 0.5986   train/acc: 0.6921   valid/acc: 0.7011
Epoch 420:   train/loss: 0.5716   valid/loss: 0.5986   train/acc: 0.6921   valid/acc: 0.7011
Epoch 430:   train/loss: 0.6454   valid/loss: 0.5986   train/acc: 0.6888   valid/acc: 0.7011
Epoch 440:   train/loss: 0.6089   valid/loss: 0.5985   train/acc: 0.6921   valid/acc: 0.7011
Epoch 450:   train/loss: 0.6224   valid/loss: 0.5984   train/acc: 0.6921   valid/acc: 0.7011
Epoch 460:   train/loss: 0.6358   valid/loss: 0.5985   train/acc: 0.6888   valid/acc: 0.7011
Epoch 470:   train/loss: 0.6557   valid/loss: 0.5986   train/acc: 0.6921   valid/acc: 0.7011
Epoch 480:   train/loss: 0.5916   valid/loss: 0.5986   train/acc: 0.6710   valid/acc: 0.7011
Epoch 490:   train/loss: 0.6187   valid/loss: 0.5982   train/acc: 0.6921   valid/acc: 0.7011
Epoch 500:   train/loss: 0.6701   valid/loss: 0.5982   train/acc: 0.6921   valid/acc: 0.7011
Epoch 510:   train/loss: 0.6310   valid/loss: 0.5979   train/acc: 0.6921   valid/acc: 0.7011
Epoch 520:   train/loss: 0.6845   valid/loss: 0.5978   train/acc: 0.6921   valid/acc: 0.7011
Epoch 530:   train/loss: 0.6121   valid/loss: 0.5977   train/acc: 0.6921   valid/acc: 0.7011
Epoch 540:   train/loss: 0.5908   valid/loss: 0.5976   train/acc: 0.6921   valid/acc: 0.7011
Epoch 550:   train/loss: 0.5799   valid/loss: 0.5976   train/acc: 0.6921   valid/acc: 0.7011
Epoch 560:   train/loss: 0.5238   valid/loss: 0.5977   train/acc: 0.6921   valid/acc: 0.7011
Epoch 570:   train/loss: 0.5946   valid/loss: 0.5976   train/acc: 0.6921   valid/acc: 0.7011
Epoch 580:   train/loss: 0.5993   valid/loss: 0.5975   train/acc: 0.6921   valid/acc: 0.7011
Epoch 590:   train/loss: 0.6073   valid/loss: 0.5974   train/acc: 0.6921   valid/acc: 0.7011
Epoch 600:   train/loss: 0.6394   valid/loss: 0.5974   train/acc: 0.6921   valid/acc: 0.7011
Epoch 610:   train/loss: 0.6057   valid/loss: 0.5975   train/acc: 0.6921   valid/acc: 0.7011
Epoch 620:   train/loss: 0.5677   valid/loss: 0.5974   train/acc: 0.6921   valid/acc: 0.7011
Epoch 630:   train/loss: 0.6626   valid/loss: 0.5971   train/acc: 0.6921   valid/acc: 0.7011
Epoch 640:   train/loss: 0.5874   valid/loss: 0.5970   train/acc: 0.6921   valid/acc: 0.7011
Epoch 650:   train/loss: 0.6691   valid/loss: 0.5970   train/acc: 0.6914   valid/acc: 0.7011
Epoch 660:   train/loss: 0.6452   valid/loss: 0.5969   train/acc: 0.6875   valid/acc: 0.7011
Epoch 670:   train/loss: 0.6281   valid/loss: 0.5967   train/acc: 0.6921   valid/acc: 0.7011
Epoch 680:   train/loss: 0.6548   valid/loss: 0.5966   train/acc: 0.6921   valid/acc: 0.7011
Epoch 690:   train/loss: 0.5706   valid/loss: 0.5965   train/acc: 0.6921   valid/acc: 0.7011
Epoch 700:   train/loss: 0.6204   valid/loss: 0.5965   train/acc: 0.6921   valid/acc: 0.7011
Epoch 710:   train/loss: 0.5657   valid/loss: 0.5964   train/acc: 0.6921   valid/acc: 0.7011
Epoch 720:   train/loss: 0.5782   valid/loss: 0.5963   train/acc: 0.6921   valid/acc: 0.7011
Epoch 730:   train/loss: 0.6020   valid/loss: 0.5963   train/acc: 0.6914   valid/acc: 0.7011
Epoch 740:   train/loss: 0.5761   valid/loss: 0.5964   train/acc: 0.6921   valid/acc: 0.7011
Epoch 750:   train/loss: 0.5871   valid/loss: 0.5964   train/acc: 0.6894   valid/acc: 0.7011
Epoch 760:   train/loss: 0.6407   valid/loss: 0.5964   train/acc: 0.6921   valid/acc: 0.7011
Epoch 770:   train/loss: 0.5560   valid/loss: 0.5961   train/acc: 0.6921   valid/acc: 0.7011
Epoch 780:   train/loss: 0.5839   valid/loss: 0.5959   train/acc: 0.6921   valid/acc: 0.7011
Epoch 790:   train/loss: 0.5660   valid/loss: 0.5958   train/acc: 0.6697   valid/acc: 0.7011
Epoch 800:   train/loss: 0.6280   valid/loss: 0.5959   train/acc: 0.6921   valid/acc: 0.7011
Epoch 810:   train/loss: 0.5755   valid/loss: 0.5960   train/acc: 0.6921   valid/acc: 0.7011
Epoch 820:   train/loss: 0.6101   valid/loss: 0.5959   train/acc: 0.6921   valid/acc: 0.7011
Epoch 830:   train/loss: 0.6147   valid/loss: 0.5961   train/acc: 0.6921   valid/acc: 0.7011
Epoch 840:   train/loss: 0.5837   valid/loss: 0.5961   train/acc: 0.6921   valid/acc: 0.7011
Epoch 850:   train/loss: 0.6554   valid/loss: 0.5958   train/acc: 0.6921   valid/acc: 0.7011
Epoch 860:   train/loss: 0.5724   valid/loss: 0.5958   train/acc: 0.6921   valid/acc: 0.7011
Epoch 870:   train/loss: 0.6187   valid/loss: 0.5958   train/acc: 0.6921   valid/acc: 0.7011
Epoch 880:   train/loss: 0.6644   valid/loss: 0.5958   train/acc: 0.6921   valid/acc: 0.7011
Epoch 890:   train/loss: 0.6767   valid/loss: 0.5957   train/acc: 0.6921   valid/acc: 0.7011
Epoch 900:   train/loss: 0.6133   valid/loss: 0.5956   train/acc: 0.6921   valid/acc: 0.7011
Epoch 910:   train/loss: 0.6320   valid/loss: 0.5956   train/acc: 0.6921   valid/acc: 0.7011
Epoch 920:   train/loss: 0.6038   valid/loss: 0.5954   train/acc: 0.6921   valid/acc: 0.7011
Epoch 930:   train/loss: 0.5771   valid/loss: 0.5954   train/acc: 0.6921   valid/acc: 0.7011
Epoch 940:   train/loss: 0.6169   valid/loss: 0.5953   train/acc: 0.6921   valid/acc: 0.7011
Epoch 950:   train/loss: 0.6564   valid/loss: 0.5952   train/acc: 0.6921   valid/acc: 0.7011
Epoch 960:   train/loss: 0.5986   valid/loss: 0.5952   train/acc: 0.6921   valid/acc: 0.7011
Epoch 970:   train/loss: 0.6511   valid/loss: 0.5950   train/acc: 0.6921   valid/acc: 0.7011
Epoch 980:   train/loss: 0.5887   valid/loss: 0.5949   train/acc: 0.6921   valid/acc: 0.7011
Epoch 990:   train/loss: 0.5985   valid/loss: 0.5947   train/acc: 0.6921   valid/acc: 0.7011
Epoch 1000:  train/loss: 0.5932   valid/loss: 0.5946   train/acc: 0.6921   valid/acc: 0.7011
Epoch 1010:  train/loss: 0.6317   valid/loss: 0.5946   train/acc: 0.6921   valid/acc: 0.7011
Epoch 1020:  train/loss: 0.6086   valid/loss: 0.5946   train/acc: 0.6921   valid/acc: 0.7011
Epoch 1030:  train/loss: 0.5900   valid/loss: 0.5946   train/acc: 0.6921   valid/acc: 0.7011
Epoch 1040:  train/loss: 0.6054   valid/loss: 0.5945   train/acc: 0.6921   valid/acc: 0.7011
Epoch 1050:  train/loss: 0.6104   valid/loss: 0.5945   train/acc: 0.6921   valid/acc: 0.7011
Epoch 1060:  train/loss: 0.5897   valid/loss: 0.5945   train/acc: 0.6921   valid/acc: 0.7011
Epoch 1070:  train/loss: 0.6393   valid/loss: 0.5946   train/acc: 0.6921   valid/acc: 0.7011
Epoch 1080:  train/loss: 0.6259   valid/loss: 0.5943   train/acc: 0.6921   valid/acc: 0.7011
Epoch 1090:  train/loss: 0.5360   valid/loss: 0.5943   train/acc: 0.6921   valid/acc: 0.7011
Epoch 1100:  train/loss: 0.5944   valid/loss: 0.5942   train/acc: 0.6921   valid/acc: 0.7011
Epoch 1110:  train/loss: 0.5506   valid/loss: 0.5940   train/acc: 0.6921   valid/acc: 0.7011
Epoch 1120:  train/loss: 0.6250   valid/loss: 0.5940   train/acc: 0.6921   valid/acc: 0.7011
Epoch 1130:  train/loss: 0.6191   valid/loss: 0.5940   train/acc: 0.6921   valid/acc: 0.7011
Epoch 1140:  train/loss: 0.6103   valid/loss: 0.5939   train/acc: 0.6901   valid/acc: 0.7011
Epoch 1150:  train/loss: 0.5837   valid/loss: 0.5936   train/acc: 0.6921   valid/acc: 0.7011
Epoch 1160:  train/loss: 0.5774   valid/loss: 0.5936   train/acc: 0.6921   valid/acc: 0.7011
Epoch 1170:  train/loss: 0.6061   valid/loss: 0.5936   train/acc: 0.6921   valid/acc: 0.7011
Epoch 1180:  train/loss: 0.5947   valid/loss: 0.5936   train/acc: 0.6921   valid/acc: 0.7011
Epoch 1190:  train/loss: 0.6230   valid/loss: 0.5934   train/acc: 0.6921   valid/acc: 0.7011
Epoch 1200:  train/loss: 0.5626   valid/loss: 0.5932   train/acc: 0.6921   valid/acc: 0.7011
Epoch 1210:  train/loss: 0.6298   valid/loss: 0.5932   train/acc: 0.6921   valid/acc: 0.7011
Epoch 1220:  train/loss: 0.5989   valid/loss: 0.5931   train/acc: 0.6921   valid/acc: 0.7011
Epoch 1230:  train/loss: 0.6039   valid/loss: 0.5930   train/acc: 0.6921   valid/acc: 0.7011
Epoch 1240:  train/loss: 0.6300   valid/loss: 0.5929   train/acc: 0.6921   valid/acc: 0.7011
Epoch 1250:  train/loss: 0.6012   valid/loss: 0.5930   train/acc: 0.6921   valid/acc: 0.7011
Epoch 1260:  train/loss: 0.6215   valid/loss: 0.5928   train/acc: 0.6921   valid/acc: 0.7011
Epoch 1270:  train/loss: 0.6534   valid/loss: 0.5927   train/acc: 0.6921   valid/acc: 0.7011
Epoch 1280:  train/loss: 0.6082   valid/loss: 0.5927   train/acc: 0.6921   valid/acc: 0.7011
Epoch 1290:  train/loss: 0.5984   valid/loss: 0.5927   train/acc: 0.6921   valid/acc: 0.7011
Epoch 1300:  train/loss: 0.6107   valid/loss: 0.5927   train/acc: 0.6921   valid/acc: 0.7011
Epoch 1310:  train/loss: 0.6067   valid/loss: 0.5927   train/acc: 0.6921   valid/acc: 0.7011
Epoch 1320:  train/loss: 0.5738   valid/loss: 0.5927   train/acc: 0.6921   valid/acc: 0.7011
Epoch 1330:  train/loss: 0.5940   valid/loss: 0.5928   train/acc: 0.6914   valid/acc: 0.7011
Epoch 1340:  train/loss: 0.5767   valid/loss: 0.5927   train/acc: 0.6875   valid/acc: 0.7011
Epoch 1350:  train/loss: 0.6266   valid/loss: 0.5925   train/acc: 0.6921   valid/acc: 0.7011
Epoch 1360:  train/loss: 0.6165   valid/loss: 0.5924   train/acc: 0.6921   valid/acc: 0.7011
Epoch 1370:  train/loss: 0.6356   valid/loss: 0.5923   train/acc: 0.6921   valid/acc: 0.7011
Epoch 1380:  train/loss: 0.5849   valid/loss: 0.5924   train/acc: 0.6914   valid/acc: 0.7011
Epoch 1390:  train/loss: 0.6543   valid/loss: 0.5923   train/acc: 0.6921   valid/acc: 0.7011
Epoch 1400:  train/loss: 0.6250   valid/loss: 0.5923   train/acc: 0.6921   valid/acc: 0.7011
Epoch 1410:  train/loss: 0.5827   valid/loss: 0.5924   train/acc: 0.6921   valid/acc: 0.7011
Epoch 1420:  train/loss: 0.6239   valid/loss: 0.5923   train/acc: 0.6921   valid/acc: 0.7011
Epoch 1430:  train/loss: 0.5771   valid/loss: 0.5923   train/acc: 0.6921   valid/acc: 0.7011
Epoch 1440:  train/loss: 0.6068   valid/loss: 0.5922   train/acc: 0.6921   valid/acc: 0.7011
Epoch 1450:  train/loss: 0.5982   valid/loss: 0.5922   train/acc: 0.6921   valid/acc: 0.7011
Epoch 1460:  train/loss: 0.5952   valid/loss: 0.5922   train/acc: 0.6921   valid/acc: 0.7011
Epoch 1470:  train/loss: 0.5664   valid/loss: 0.5922   train/acc: 0.6921   valid/acc: 0.7011
Epoch 1480:  train/loss: 0.6654   valid/loss: 0.5922   train/acc: 0.6921   valid/acc: 0.7011
Epoch 1490:  train/loss: 0.6230   valid/loss: 0.5923   train/acc: 0.6921   valid/acc: 0.7011
Epoch 1500:  train/loss: 0.6556   valid/loss: 0.5923   train/acc: 0.6855   valid/acc: 0.7011

Training completed!
ic| main(EPOCHS, SEED, BATCH_SIZE,MODEL=model): None
ic| 'setting model'
ic| ic(EPOCHS: 1500, SEED: 450, BATCH_SIZE: 700
Train circuits done
Dev circuits done
Test circuits done
OOV circuits done
Redundant circuits done
Initialize trainer
2024-08-08_11_13_54
Starting fit
Saving everything
Initialize trainer
2024-08-08_17_04_09
Starting fit
Traceback (most recent call last):
  File "/home/jrubiope/FslQnlp/MC_exe.py", line 426, in <module>
    ic(main(EPOCHS, SEED, BATCH_SIZE,MODEL=model))
       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jrubiope/FslQnlp/MC_exe.py", line 376, in main
    trainer.fit(train_dataset, val_dataset, log_interval=10)
  File "/home/jrubiope/.local/lib/python3.11/site-packages/lambeq/training/quantum_trainer.py", line 200, in fit
    super().fit(train_dataset,
  File "/home/jrubiope/.local/lib/python3.11/site-packages/lambeq/training/trainer.py", line 435, in fit
    t_loss = self._step_and_eval(
             ^^^^^^^^^^^^^^^^^^^^
  File "/home/jrubiope/.local/lib/python3.11/site-packages/lambeq/training/trainer.py", line 341, in _step_and_eval
    y_hat, loss = step_func(batch)
                  ^^^^^^^^^^^^^^^^
  File "/home/jrubiope/.local/lib/python3.11/site-packages/lambeq/training/quantum_trainer.py", line 163, in training_step
    loss = self.optimizer.backward(batch)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jrubiope/.local/lib/python3.11/site-packages/lambeq/training/spsa_optimizer.py", line 152, in backward
    y0 = self.model(diagrams)
         ^^^^^^^^^^^^^^^^^^^^
  File "/home/jrubiope/.local/lib/python3.11/site-packages/lambeq/training/quantum_model.py", line 147, in __call__
    out = self.forward(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jrubiope/.local/lib/python3.11/site-packages/lambeq/training/numpy_model.py", line 192, in forward
    return self.get_diagram_output(x)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jrubiope/.local/lib/python3.11/site-packages/lambeq/training/numpy_model.py", line 158, in get_diagram_output
    res: jnp.ndarray = jnp.array([diag_f(self.weights)
                                 ^^^^^^^^^^^^^^^^^^^^^
  File "/home/jrubiope/.local/lib/python3.11/site-packages/lambeq/training/numpy_model.py", line 158, in <listcomp>
    res: jnp.ndarray = jnp.array([diag_f(self.weights)
                                  ^^^^^^^^^^^^^^^^^^^^
jaxlib.xla_extension.XlaRuntimeError: INTERNAL: Failed to launch ptxas: If the error message indicates that a file could not be written, please verify that sufficient filesystem space is provided.
--------------------
For simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these.
Thu  8 Aug 17:06:15 BST 2024
GPU Epilog Script v0.30
