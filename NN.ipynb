{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lambeq import CircuitAnsatz\n",
    "from discopy.quantum.gates import Ry, Rx, Rz, CRx\n",
    "from discopy.quantum import (\n",
    "    Bra,\n",
    "    Circuit,\n",
    "    CRz,\n",
    "    Discard,\n",
    "    H,\n",
    "    Id,\n",
    "    Ket,\n",
    "    qubit,\n",
    "    Rx, Ry, Rz\n",
    ")\n",
    "from discopy.quantum.circuit import Functor\n",
    "from discopy.grammar.pregroup import Box, Category, Diagram, Ty\n",
    "from sympy import Symbol\n",
    "\n",
    "\n",
    "class FslNN(CircuitAnsatz):\n",
    "    mappings=[]\n",
    "    def __init__(self, ob_map, n_layers, n_single_qubit_params = 2, discard = False):\n",
    "        self.preq_embeddings=preq_embeddings\n",
    "        super().__init__(ob_map,\n",
    "                         n_layers,\n",
    "                         n_single_qubit_params,\n",
    "                         self.circuito,\n",
    "                         discard,\n",
    "                         [Rx])\n",
    "        \n",
    "\n",
    "                         \n",
    "    def __call__(self,diagram,maps,width,initial=False):\n",
    "        return self.circuito(width=width,params=maps,initial=initial)\n",
    "\n",
    "    def params_shape(self, n_qubits):\n",
    "        return (self.n_layers + 1, n_qubits)\n",
    "\n",
    "    def circuito(self,width,params,initial):\n",
    "\n",
    "        circuit=Ket(*np.zeros(width, dtype=int))\n",
    "\n",
    "        if initial:\n",
    "            circuit >>= Id().tensor(*[Ry(Symbol('initialize')) for i in range(width)])\n",
    "            return circuit\n",
    "\n",
    "    \n",
    "\n",
    "        circuit >>= Id().tensor(*[Ry(phase=params[i]) for i in range(width)])\n",
    "        circuit >>= Id().tensor(*[Rz(phase=params[i]) for i in range(width,2*width)])\n",
    "        for j in range(width - 1):\n",
    "            circuit >>= Id(j) @ CRx(phase=params[j+2*width]) @ Id(width - j - 2)\n",
    "\n",
    "\n",
    "        \n",
    "        return circuit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn,matmul,kron,bmm\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "#from lambeq import BobcatParser, AtomicType, SpacyTokeniser, Rewriter\n",
    "#from lambeq import IQPAnsatz,Sim15Ansatz\n",
    "# from utils.FslAnsatzNN import FslSim15Ansatz, FslStronglyEntanglingAnsatz, FslBaseAnsatz\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision.transforms as transforms\n",
    "#from lambeq import NumpyModel,PytorchModel\n",
    "#from lambeq import IQPAnsatz,Sim15Ansatz\n",
    "from datetime import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from icecream import ic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self,labels,embeddings):\n",
    "        self.labels=labels\n",
    "        self.embeddings=embeddings\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        return self.embeddings[idx],self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "preq_embeddings={}\n",
    "with open(\"resources\\embeddings\\common_crawl\\glove.42B.300d.txt\", 'r', encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], \"double\")\n",
    "        preq_embeddings[word] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TESTING=True\n",
    "\n",
    "words=[]\n",
    "embeddings=[]\n",
    "for key, value in preq_embeddings.items():\n",
    "    words.append(key)\n",
    "    embeddings.append(value)\n",
    "\n",
    "if TESTING:\n",
    "    training_words=words[:30]\n",
    "    training_embeddings=embeddings[:30]\n",
    "\n",
    "    dev_words=words[31:60]\n",
    "    dev_embeddings=embeddings[31:60]\n",
    "\n",
    "# training_embeddings=torch.tensor(  np.array([np.reshape(embedding,(1,300)) for embedding in training_embeddings]) ,requires_grad=True )\n",
    "# dev_embeddings=torch.tensor(np.array([np.reshape(embedding,(1,300)) for embedding in dev_embeddings]),requires_grad=True)\n",
    "\n",
    "training_embeddings=[np.reshape(embedding,(1,300)) for embedding in training_embeddings]\n",
    "dev_embeddings=[np.reshape(embedding,(1,300)) for embedding in dev_embeddings]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data(embeddings):\n",
    "    training_data=[]\n",
    "    training_labels=[]\n",
    "    for i in range(len(embeddings)):\n",
    "        for j in range(len(embeddings)):\n",
    "\n",
    "            training_labels.append(np.inner(embeddings[i],embeddings[j])[0][0])\n",
    "\n",
    "            new_embedding=np.append(embeddings[i],embeddings[j])\n",
    "\n",
    "            training_data.append(new_embedding)\n",
    "\n",
    "    training_data=torch.tensor(training_data,requires_grad=True)\n",
    "\n",
    "    training_labels=torch.tensor(training_labels,requires_grad=True)\n",
    "\n",
    "    return training_data, training_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_gate=torch.tensor([[0,-1j],[1j,0]],dtype=torch.complex128,requires_grad=True)\n",
    "x_gate=torch.tensor([[0,1],[1,0]],dtype=torch.complex128,requires_grad=True)\n",
    "z_gate=torch.tensor([[1,0],[0,-1]],dtype=torch.complex128,requires_grad=True)\n",
    "\n",
    "def Id():\n",
    "    gate=torch.tensor([[1.+0j,0],[0,1.+0j]],requires_grad=True,dtype=torch.complex128)\n",
    "    gate.retain_grad()\n",
    "    return gate\n",
    "\n",
    "def CRx(theta):\n",
    "    gate=torch.tensor( [ [1.,0,0,0], [0, 1. ,0,0] ,[0,0,torch.cos(theta/2), -1j*torch.sin(theta/2) ] , [0,0,-1j*torch.sin(theta/2) , torch.cos(theta/2)] ]   ,requires_grad=True)   \n",
    "    gate.retain_grad()\n",
    "    return gate\n",
    "\n",
    "def zero_bra():\n",
    "    gate=torch.tensor([1.+0j,0,0,0,0,0,0,0],requires_grad=True,dtype=torch.complex128)\n",
    "    gate.retain_grad()\n",
    "    return gate\n",
    "\n",
    "def zero_ket():\n",
    "    gate=torch.tensor([[1.+0j],[0],[0],[0],[0],[0],[0],[0]],requires_grad=True,dtype=torch.complex128)\n",
    "    gate.retain_grad()\n",
    "    return gate\n",
    "\n",
    "def zero_1d_ket():\n",
    "    gate=torch.tensor([[1.+0j],[0]],requires_grad=True,dtype=torch.complex128)\n",
    "    gate.retain_grad()\n",
    "    return gate\n",
    "\n",
    "def zero_1d_bra():\n",
    "    gate=torch.tensor([[1.+0j,0]],requires_grad=True,dtype=torch.complex128)\n",
    "    gate.retain_grad()\n",
    "    return gate\n",
    "\n",
    "def one_1d_ket():\n",
    "    gate=torch.tensor([[0],[1.+0j]],requires_grad=True,dtype=torch.complex128)\n",
    "    gate.retain_grad()\n",
    "    return gate\n",
    "\n",
    "def one_1d_bra():\n",
    "    gate=torch.tensor([[0,1.+0j]],requires_grad=True,dtype=torch.complex128)\n",
    "    gate.retain_grad()\n",
    "    return gate\n",
    "\n",
    "def Ry(theta):\n",
    "    gate = torch.linalg.matrix_exp(-0.5j*theta[:,:,None]*y_gate)\n",
    "    return gate\n",
    "\n",
    "def Rx(theta):\n",
    "    gate = torch.linalg.matrix_exp(-0.5j*theta[:,:,None]*x_gate)\n",
    "    return gate\n",
    "\n",
    "def Rx(theta):\n",
    "    gate = torch.linalg.matrix_exp(-0.5j*theta[:,:,None]*z_gate)\n",
    "    return gate\n",
    "\n",
    "#Rx=lambda x: torch.exp(-0.5j*x*torch.tensor([[0,1],[1,0]],dtype=torch.complex128))\n",
    "\n",
    "#Ry=lambda x: torch.exp(-0.5j*x*torch.tensor([[0,-1j],[1j,0]],dtype=torch.complex128))\n",
    "\n",
    "CRx=lambda x: kron(Id(),matmul(zero_1d_ket(),zero_1d_bra()))+kron(Rx(x),matmul(one_1d_ket(),one_1d_bra()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s=torch.tensor([[1.,2,3,4],[1.,2,3,4],[1.,2,3,4]],requires_grad=True)\n",
    "n=torch.tensor([[0,1]  ,[1.,0]],requires_grad=True)\n",
    "s2=torch.reshape(s,(3,4,1))\n",
    "s3=s2[:,:,None]*n\n",
    "print(s3.shape)\n",
    "torch.linalg.matrix_exp(1j*0.5*np.pi*s3).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ucl_l\\AppData\\Local\\Temp\\ipykernel_20716\\535721322.py:13: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ..\\torch\\csrc\\utils\\tensor_new.cpp:277.)\n",
      "  training_data=torch.tensor(training_data,requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "training_data,training_labels=create_data(training_embeddings)\n",
    "dev_data,dev_labels=create_data(dev_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_object=CustomDataset(training_labels,training_data)\n",
    "dev_object=CustomDataset(dev_labels,dev_data)\n",
    "\n",
    "training_loader=DataLoader(training_object,batch_size=15)\n",
    "validation_loader=DataLoader(dev_object,batch_size=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model and training setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss_fn=lambda x,y:np.abs(x-y)\n",
    "# def loss_fn(output,target):\n",
    "#     print(output,target)\n",
    "#     return nn.MSELoss(output,target)\n",
    "loss_fn = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_PARAMS=8\n",
    "class PreQ(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten(start_dim=0)\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(300, 200),\n",
    "            nn.ReLU(),\n",
    "            # nn.Linear(300, 250),\n",
    "            # nn.ReLU(),\n",
    "            # nn.Linear(250, 200),\n",
    "            #nn.ReLU(),\n",
    "            nn.Linear(200, 50),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(50,N_PARAMS)\n",
    "        )    \n",
    "        self.double()\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x = self.flatten(x)\n",
    "        x.requires_grad_()\n",
    "        x.retain_grad()\n",
    "        logits1=self.linear_relu_stack(x[:,0:300])\n",
    "        logits2=self.linear_relu_stack(x[:,300:600])\n",
    "\n",
    "        logits1_reshaped=torch.reshape(logits1,(logits1.shape[0],logits1.shape[1],1))\n",
    "        logits2_reshaped=torch.reshape(logits2,(logits2.shape[0],logits2.shape[1],1))\n",
    "        \n",
    "        bra=torch.stack([zero_bra()[None] for i in range(logits1_reshaped.shape[0])])\n",
    "        ket=torch.stack([zero_ket() for i in range(logits1_reshaped.shape[0])])\n",
    "\n",
    "        circuit1=bmm(bra,self.get_quantum_state(parameters=logits2_reshaped).mH)\n",
    "        circuit2=bmm(self.get_quantum_state(parameters=logits1_reshaped),ket)\n",
    "\n",
    "        output=self.flatten(bmm(circuit1,circuit2))\n",
    "\n",
    "        return output\n",
    "\n",
    "    def get_quantum_state(self,parameters):\n",
    "        first_layer=torch.stack( [   kron(kron( Rx(parameters[:,0])[i],Rx(parameters[:,1])[i] ),Rx(parameters[:,2])[i] )  for i in range(parameters.shape[0])   ]   )\n",
    "        second_layer=torch.stack( [   kron(kron( Ry(parameters[:,0])[i],Ry(parameters[:,1])[i] ),Ry(parameters[:,2])[i] )  for i in range(parameters.shape[0])   ]  )\n",
    "        \n",
    "        third_layer=kron(CRx(parameters[:,6]),Id())\n",
    "        fourth_layer=kron(Id(),CRx(parameters[:,7]))\n",
    "        \n",
    "        output=bmm(bmm(bmm(first_layer,second_layer),third_layer),fourth_layer)\n",
    "        return output\n",
    "\n",
    "model = PreQ().to(device)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizers specified in the torch.optim package\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| logits1: tensor([[-0.1297, -0.1242, -0.1167,  0.0371, -0.0249, -0.0699, -0.1014,  0.1559],\n",
      "                     [-0.2436, -0.1266,  0.0152,  0.0252, -0.0543, -0.0907, -0.2159,  0.0193],\n",
      "                     [-0.1705, -0.1355, -0.0463, -0.1054, -0.0694, -0.1704, -0.0896,  0.0599]],\n",
      "                    dtype=torch.float64, grad_fn=<AddmmBackward0>)\n",
      "C:\\Users\\ucl_l\\AppData\\Local\\Temp\\ipykernel_20716\\118236819.py:14: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at aten\\src\\ATen/core/TensorBody.h:494.)\n",
      "  ic(loss.grad)\n",
      "ic| loss.grad: None\n"
     ]
    }
   ],
   "source": [
    "optimizer.zero_grad()\n",
    "\n",
    "x = torch.randn(3,600,requires_grad = True,dtype=torch.double)\n",
    "# x=training_data\n",
    "y = model(x)\n",
    "\n",
    "y=y.type(torch.DoubleTensor)\n",
    "\n",
    "z=torch.randn(y.shape,requires_grad=True,dtype=torch.double)\n",
    "z.retain_grad()\n",
    "\n",
    "loss=loss_fn(y,z)\n",
    "loss.backward()\n",
    "ic(loss.grad)\n",
    "\n",
    "optimizer.step()\n",
    "\n",
    "\n",
    "# for name, param in model.named_parameters():\n",
    "#     print(name, param.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch_index, tb_writer):\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "\n",
    "    for i, data in enumerate(training_loader):\n",
    "        # Every data instance is an input + label pair\n",
    "        inputs, labels = data\n",
    "\n",
    "\n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Make predictions for this batch\n",
    "        \n",
    "        #ic(inputs,labels)\n",
    "        output = model(inputs)\n",
    "\n",
    "        output=output.to(dtype=torch.double)\n",
    "\n",
    "\n",
    "        # Compute the loss and its gradients\n",
    "        loss = loss_fn(output, labels)\n",
    "        ic(loss.grad)\n",
    "        loss.backward()\n",
    "        \n",
    "\n",
    "        # for name, param in model.named_parameters():\n",
    "        #     print(name, param.grad)\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "        if i % 15 == 14:\n",
    "            last_loss = running_loss / 15 # loss per batch\n",
    "            print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "            tb_x = epoch_index * len(training_loader) + i + 1\n",
    "            tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
    "            running_loss = 0.\n",
    "\n",
    "    return last_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing in a separate cell so we can easily add more epochs to the same run\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "writer = SummaryWriter('runs/Prueba{}'.format(timestamp))\n",
    "epoch_number = 0\n",
    "EPOCHS = 1\n",
    "best_vloss = 1_000_000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ucl_l\\AppData\\Local\\Temp\\ipykernel_20716\\1488263493.py:23: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at aten\\src\\ATen/core/TensorBody.h:494.)\n",
      "  ic(loss.grad)\n",
      "ic| loss.grad: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| loss.grad: None\n",
      "ic| loss.grad: None\n",
      "ic| loss.grad: None\n",
      "ic| loss.grad: None\n",
      "ic| loss.grad: None\n",
      "ic| loss.grad: None\n",
      "ic| loss.grad: None\n",
      "ic| loss.grad: None\n",
      "ic| loss.grad: None\n",
      "ic| loss.grad: None\n",
      "ic| loss.grad: None\n",
      "ic| loss.grad: None\n",
      "ic| loss.grad: None\n",
      "ic| loss.grad: None\n",
      "ic| loss.grad: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 15 loss: 1528.4703182625672\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[64], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Make sure gradient tracking is on, and do a pass over the data\u001b[39;00m\n\u001b[0;32m      5\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m----> 6\u001b[0m avg_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch_number\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m running_vloss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Set the model to evaluation mode, disabling dropout and using population\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# statistics for batch normalization.\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[62], line 16\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[1;34m(epoch_index, tb_writer)\u001b[0m\n\u001b[0;32m     11\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Make predictions for this batch\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m#ic(inputs,labels)\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m output\u001b[38;5;241m=\u001b[39moutput\u001b[38;5;241m.\u001b[39mto(dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdouble)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Compute the loss and its gradients\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ucl_l\\Documents\\GitHub\\FslQNLP\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ucl_l\\Documents\\GitHub\\FslQNLP\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[61], line 35\u001b[0m, in \u001b[0;36mPreQ.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     32\u001b[0m ket\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mstack([zero_ket() \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(logits1\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])])\n\u001b[0;32m     34\u001b[0m circuit1\u001b[38;5;241m=\u001b[39mbmm(bra,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_quantum_state(parameters\u001b[38;5;241m=\u001b[39mlogits2)\u001b[38;5;241m.\u001b[39mmH)\n\u001b[1;32m---> 35\u001b[0m circuit2\u001b[38;5;241m=\u001b[39mbmm(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_quantum_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits1\u001b[49m\u001b[43m)\u001b[49m,ket)\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflatten(bmm(circuit1,circuit2))\n",
      "Cell \u001b[1;32mIn[61], line 41\u001b[0m, in \u001b[0;36mPreQ.get_quantum_state\u001b[1;34m(self, parameters)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_quantum_state\u001b[39m(\u001b[38;5;28mself\u001b[39m,parameters):\n\u001b[0;32m     40\u001b[0m     first_layer\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mstack( [   kron(kron( Rx(parameters[:,\u001b[38;5;241m0\u001b[39m])[i],Rx(parameters[:,\u001b[38;5;241m1\u001b[39m])[i] ),Rx(parameters[:,\u001b[38;5;241m2\u001b[39m])[i] )  \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(parameters\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])   ]   )\n\u001b[1;32m---> 41\u001b[0m     second_layer\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mstack( \u001b[43m[\u001b[49m\u001b[43m   \u001b[49m\u001b[43mkron\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkron\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[43mRy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mRy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mRy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m   \u001b[49m\u001b[43m]\u001b[49m  )\n\u001b[0;32m     43\u001b[0m     third_layer\u001b[38;5;241m=\u001b[39mkron(CRx(parameters[:,\u001b[38;5;241m6\u001b[39m]),Id())\n\u001b[0;32m     44\u001b[0m     fourth_layer\u001b[38;5;241m=\u001b[39mkron(Id(),CRx(parameters[:,\u001b[38;5;241m7\u001b[39m]))\n",
      "Cell \u001b[1;32mIn[61], line 41\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_quantum_state\u001b[39m(\u001b[38;5;28mself\u001b[39m,parameters):\n\u001b[0;32m     40\u001b[0m     first_layer\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mstack( [   kron(kron( Rx(parameters[:,\u001b[38;5;241m0\u001b[39m])[i],Rx(parameters[:,\u001b[38;5;241m1\u001b[39m])[i] ),Rx(parameters[:,\u001b[38;5;241m2\u001b[39m])[i] )  \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(parameters\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])   ]   )\n\u001b[1;32m---> 41\u001b[0m     second_layer\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mstack( [   kron(kron( Ry(parameters[:,\u001b[38;5;241m0\u001b[39m])[i],Ry(parameters[:,\u001b[38;5;241m1\u001b[39m])[i] ),\u001b[43mRy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m[i] )  \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(parameters\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])   ]  )\n\u001b[0;32m     43\u001b[0m     third_layer\u001b[38;5;241m=\u001b[39mkron(CRx(parameters[:,\u001b[38;5;241m6\u001b[39m]),Id())\n\u001b[0;32m     44\u001b[0m     fourth_layer\u001b[38;5;241m=\u001b[39mkron(Id(),CRx(parameters[:,\u001b[38;5;241m7\u001b[39m]))\n",
      "Cell \u001b[1;32mIn[6], line 46\u001b[0m, in \u001b[0;36mRy\u001b[1;34m(theta)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mRy\u001b[39m(theta):\n\u001b[1;32m---> 46\u001b[0m     gate \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatrix_exp\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43mj\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtheta\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m]\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43my_gate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m gate\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    print('EPOCH {}:'.format(epoch_number + 1))\n",
    "\n",
    "    # Make sure gradient tracking is on, and do a pass over the data\n",
    "    model.train(True)\n",
    "    avg_loss = train_one_epoch(epoch_number, writer)\n",
    "\n",
    "\n",
    "    running_vloss = 0.0\n",
    "    # Set the model to evaluation mode, disabling dropout and using population\n",
    "    # statistics for batch normalization.\n",
    "    model.eval()\n",
    "\n",
    "    # Disable gradient computation and reduce memory consumption.\n",
    "    with torch.no_grad():\n",
    "        for i, vdata in enumerate(validation_loader):\n",
    "            vinputs, vlabels = vdata\n",
    "\n",
    "            voutputs = model(vinputs)\n",
    "            voutputs=voutputs.to(dtype=torch.double)\n",
    "\n",
    "            vloss = loss_fn(voutputs, vlabels)\n",
    "            running_vloss += vloss\n",
    "\n",
    "    avg_vloss = running_vloss / (i + 1)\n",
    "    print('LOSS-- Train: {} Valid: {}'.format(avg_loss, avg_vloss))\n",
    "\n",
    "    # Log the running loss averaged per batch\n",
    "    # for both training and validation\n",
    "    writer.add_scalars('Training vs. Validation Loss',\n",
    "                    { 'Training' : avg_loss, 'Validation' : avg_vloss },\n",
    "                    epoch_number + 1)\n",
    "    writer.flush()\n",
    "\n",
    "    # Track best performance, and save the model's state\n",
    "    if avg_vloss < best_vloss:\n",
    "        best_vloss = avg_vloss\n",
    "        model_path = 'runs/NN/model_{}_{}'.format(timestamp, epoch_number)\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    epoch_number += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear_relu_stack.0.weight tensor([[ 6.8447e-04, -2.5461e-04,  1.2643e-05,  ...,  6.6182e-04,\n",
      "         -5.8039e-04, -7.9825e-04],\n",
      "        [-2.9097e-04,  3.8870e-04,  1.9614e-04,  ..., -2.6235e-04,\n",
      "          4.4786e-04,  9.1579e-04],\n",
      "        [-9.5257e-04,  7.4502e-04,  2.2726e-04,  ..., -4.2169e-04,\n",
      "          6.8613e-04,  1.3927e-03],\n",
      "        ...,\n",
      "        [ 7.8595e-05, -6.7144e-06, -1.5655e-04,  ...,  1.2636e-05,\n",
      "         -8.1359e-05,  8.7035e-05],\n",
      "        [-3.0178e-05,  1.8431e-04,  1.6299e-04,  ..., -6.3107e-05,\n",
      "          1.6870e-04,  2.5272e-04],\n",
      "        [ 1.8042e-05,  3.0062e-05, -2.2960e-05,  ..., -4.1022e-05,\n",
      "         -3.2452e-05,  2.8035e-05]], dtype=torch.float64)\n",
      "linear_relu_stack.0.bias tensor([-2.4744e-03, -2.7003e-05,  7.1029e-04, -1.5306e-03,  7.4028e-05,\n",
      "         0.0000e+00, -1.9793e-03, -2.0840e-03,  4.4033e-04,  1.7426e-03,\n",
      "         2.5941e-04,  5.8927e-04, -8.0517e-05,  7.3306e-05, -3.0759e-04,\n",
      "         0.0000e+00, -4.3708e-04,  3.4158e-03, -1.4465e-03, -9.2754e-04,\n",
      "         3.1100e-04, -1.9250e-03,  1.4854e-03,  8.0802e-04,  2.6191e-04,\n",
      "        -1.3891e-03,  0.0000e+00,  2.5141e-04, -1.3503e-03, -1.3321e-03,\n",
      "         1.1677e-03,  3.2432e-03, -8.8072e-04,  9.3434e-04, -1.5379e-03,\n",
      "        -4.5426e-04,  2.0840e-03, -9.3539e-04,  6.6756e-05, -6.3693e-04,\n",
      "         4.1749e-03, -1.4466e-04,  0.0000e+00, -1.2838e-04, -6.7985e-04,\n",
      "        -2.5432e-03,  2.3500e-03,  1.8414e-03, -1.6409e-03, -1.0200e-04,\n",
      "        -3.4652e-05,  0.0000e+00,  0.0000e+00, -1.1684e-03,  1.9881e-03,\n",
      "         3.7051e-06,  5.3027e-04,  3.9780e-04,  1.0902e-04,  1.3035e-04,\n",
      "         8.7516e-04, -7.8575e-04,  0.0000e+00,  1.1040e-03, -5.0097e-03,\n",
      "        -1.2323e-03,  4.9351e-03, -2.7782e-03,  1.3721e-03, -2.8048e-03,\n",
      "         6.1910e-04,  0.0000e+00,  1.3900e-04,  1.5918e-03, -5.1288e-04,\n",
      "         6.8808e-05, -2.0219e-03, -3.1414e-04,  3.2733e-04, -8.5700e-04,\n",
      "         1.3364e-03, -1.9670e-03, -1.5119e-04, -2.8433e-04,  1.4939e-04,\n",
      "         8.3922e-05,  0.0000e+00,  0.0000e+00, -2.1816e-04, -3.2466e-03,\n",
      "        -2.1850e-05, -3.9847e-05, -8.2068e-04,  1.3780e-03,  2.8844e-04,\n",
      "         2.7234e-04,  0.0000e+00,  1.3554e-04,  2.2574e-03, -4.7875e-04,\n",
      "        -1.0402e-03, -7.7938e-05,  1.9031e-04, -3.6758e-04, -2.2661e-03,\n",
      "        -1.6309e-03, -5.0539e-04,  3.0931e-03, -9.2205e-04, -1.3316e-03,\n",
      "        -1.0408e-03,  3.3383e-04,  1.9049e-03,  0.0000e+00,  9.2187e-04,\n",
      "         0.0000e+00,  5.5653e-03,  6.1321e-04,  1.1824e-04,  9.9554e-06,\n",
      "        -6.9667e-04, -8.8405e-04, -2.7614e-04, -1.6157e-03,  8.4629e-04,\n",
      "         0.0000e+00,  8.2388e-05,  1.5487e-03, -6.7556e-04, -1.6121e-03,\n",
      "         8.8691e-04, -9.4793e-03,  2.8315e-03,  2.1069e-03, -6.0711e-04,\n",
      "         0.0000e+00, -3.5586e-04,  8.5254e-05,  0.0000e+00, -8.3613e-04,\n",
      "         3.0353e-04,  1.4636e-03,  5.5757e-04,  3.0716e-03,  3.9206e-05,\n",
      "        -7.3766e-04, -1.3584e-03,  0.0000e+00,  0.0000e+00, -9.6809e-04,\n",
      "        -1.9777e-05,  2.7685e-04,  2.6215e-03,  0.0000e+00, -1.8793e-03,\n",
      "         4.2727e-04,  1.0052e-04,  2.6058e-03,  1.4453e-04, -2.4009e-04,\n",
      "         0.0000e+00, -3.1049e-04, -1.6136e-03, -1.2524e-03,  5.8955e-04,\n",
      "         1.4605e-03, -1.8917e-03, -1.2211e-04,  0.0000e+00,  1.6692e-03,\n",
      "        -7.2049e-04, -2.1853e-04, -1.6222e-03, -1.8967e-03,  2.2975e-03,\n",
      "         0.0000e+00,  1.5790e-03,  2.6493e-04,  8.9647e-04, -1.5814e-03,\n",
      "         2.4850e-03, -5.5812e-04,  7.1929e-04,  4.4176e-03,  6.1118e-04,\n",
      "        -8.8213e-04,  0.0000e+00, -2.6617e-04, -1.2843e-03,  3.0829e-03,\n",
      "        -1.0770e-03,  6.9703e-04,  0.0000e+00, -1.2088e-04,  8.7281e-05,\n",
      "         9.2876e-04, -8.6274e-04, -6.0533e-04, -5.4098e-04,  8.9725e-05],\n",
      "       dtype=torch.float64)\n",
      "linear_relu_stack.2.weight tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [-4.8473e-04, -1.2659e-03, -2.9019e-03,  ..., -1.4944e-03,\n",
      "         -1.6778e-03, -5.8417e-04],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        ...,\n",
      "        [ 0.0000e+00, -8.2446e-05, -1.6431e-04,  ..., -9.8188e-05,\n",
      "         -3.6051e-05,  0.0000e+00],\n",
      "        [ 3.1463e-05, -2.2164e-03, -1.5595e-03,  ...,  2.7806e-03,\n",
      "         -9.3363e-04,  1.5503e-04],\n",
      "        [ 4.6009e-05, -1.0887e-03, -5.4719e-04,  ...,  2.0602e-03,\n",
      "         -5.5817e-04,  4.7542e-04]], dtype=torch.float64)\n",
      "linear_relu_stack.2.bias tensor([ 0.0000e+00, -7.6569e-03,  0.0000e+00,  0.0000e+00,  9.7488e-03,\n",
      "        -1.6316e-03, -7.8057e-03, -3.6222e-03, -8.9617e-04,  4.2549e-03,\n",
      "         0.0000e+00, -2.0060e-03,  1.2399e-05, -1.9041e-03, -5.7740e-03,\n",
      "         2.6940e-04,  3.4230e-03, -7.0091e-06,  4.7950e-04, -1.1285e-02,\n",
      "        -5.1290e-03,  7.1106e-03,  0.0000e+00, -1.3241e-03,  0.0000e+00,\n",
      "         0.0000e+00,  1.4951e-03,  2.0241e-03, -2.3933e-03,  2.9821e-03,\n",
      "         2.6190e-03,  3.4986e-06,  3.2687e-03,  5.4013e-06,  0.0000e+00,\n",
      "         0.0000e+00,  1.5559e-02,  4.3683e-05,  1.5090e-03,  3.3991e-03,\n",
      "        -6.3892e-06, -1.3377e-04,  1.1529e-05,  3.8259e-03, -1.4723e-05,\n",
      "         2.8773e-04, -2.2674e-03, -4.7925e-04, -3.1233e-03,  8.3593e-06],\n",
      "       dtype=torch.float64)\n",
      "linear_relu_stack.4.weight tensor([[ 0.0000e+00, -5.7405e-03,  0.0000e+00,  0.0000e+00, -3.4943e-03,\n",
      "         -4.3435e-04, -4.7238e-03, -1.2398e-03, -4.6054e-04, -7.9289e-05,\n",
      "          0.0000e+00, -1.3983e-03, -5.5378e-03, -6.8041e-04, -1.1834e-03,\n",
      "          2.1109e-02, -1.5169e-03, -8.4328e-03, -1.7766e-05, -2.2914e-03,\n",
      "         -3.9975e-03,  7.8705e-03,  0.0000e+00, -1.6040e-04,  0.0000e+00,\n",
      "          0.0000e+00, -7.2299e-04, -1.1724e-03, -6.5272e-04, -3.3403e-03,\n",
      "         -1.6612e-03,  1.7898e-03, -3.8163e-03, -2.8021e-03,  0.0000e+00,\n",
      "          0.0000e+00, -4.3945e-03, -3.4555e-03, -1.6210e-03, -1.8605e-03,\n",
      "          3.4544e-03, -2.4372e-03, -8.2801e-03, -6.1093e-04,  7.1407e-03,\n",
      "         -7.4568e-03, -2.4369e-03, -3.4918e-04,  1.4945e-03, -3.0356e-03],\n",
      "        [ 0.0000e+00,  3.1064e-03,  0.0000e+00,  0.0000e+00,  3.0021e-03,\n",
      "          3.0718e-04,  3.2374e-03,  6.6078e-04,  3.1061e-04,  7.6660e-05,\n",
      "          0.0000e+00,  1.1864e-03,  3.7809e-03,  3.6593e-04,  3.4442e-04,\n",
      "         -1.2598e-02,  1.0983e-03,  2.7434e-03,  7.4981e-06,  1.1381e-03,\n",
      "          1.7989e-03, -4.9050e-03,  0.0000e+00, -3.2094e-05,  0.0000e+00,\n",
      "          0.0000e+00,  4.0347e-04,  5.9400e-04,  4.5318e-04,  3.8055e-04,\n",
      "          6.6874e-04, -2.0528e-03,  2.5942e-03,  2.1070e-03,  0.0000e+00,\n",
      "          0.0000e+00,  2.8709e-03,  2.2005e-03,  8.2647e-04,  8.9051e-04,\n",
      "         -2.9130e-03,  1.3881e-03,  5.1034e-03,  4.0999e-04, -3.8623e-03,\n",
      "          4.7678e-03,  1.3212e-03,  2.3154e-04, -3.6577e-04, -1.9101e-04],\n",
      "        [ 0.0000e+00,  2.0912e-03,  0.0000e+00,  0.0000e+00,  1.6712e-03,\n",
      "          3.0913e-04,  2.0866e-03,  1.3554e-04,  3.7516e-04,  3.3990e-05,\n",
      "          0.0000e+00,  6.7335e-04,  2.9783e-03,  3.6228e-04,  3.5552e-04,\n",
      "         -9.3375e-03,  5.8809e-04,  2.2026e-03,  7.5039e-06, -2.1476e-04,\n",
      "          1.7062e-03, -4.2783e-03,  0.0000e+00, -2.2361e-05,  0.0000e+00,\n",
      "          0.0000e+00,  2.9817e-04,  6.1778e-04,  4.6077e-04,  1.9017e-04,\n",
      "          9.8693e-05, -1.2485e-03,  1.5467e-03,  1.1462e-03,  0.0000e+00,\n",
      "          0.0000e+00,  2.4968e-03,  1.1964e-03,  3.7818e-04,  6.4575e-04,\n",
      "         -2.3842e-03,  5.8119e-04,  3.1661e-03,  3.1418e-04, -2.5860e-03,\n",
      "          2.8670e-03,  1.2311e-03,  4.7972e-05,  2.3954e-04,  1.2080e-03],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "       dtype=torch.float64)\n",
      "linear_relu_stack.4.bias tensor([-4.3492e-05,  6.3574e-05,  5.2841e-05,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name, param.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
