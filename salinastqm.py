# -*- coding: utf-8 -*-
"""salinastqm.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1piNJp1Ersb0aqpibBBi6pY09ZYsp3DZn
"""

import os
import torch
from torch import nn,matmul,kron,bmm
from torch.utils.data import DataLoader
import numpy as np
from torchvision import datasets, transforms
import torchvision.transforms as transforms
from datetime import datetime
from torch.utils.tensorboard import SummaryWriter


from numpy import dot
from numpy.linalg import norm

#from google.colab import drive
#drive.mount('/content/drive')

device = (
    "cuda"
    if torch.cuda.is_available()
    else "mps"
    if torch.backends.mps.is_available()
    else "cpu"
)

print(f"Using {device} device")

from torch.utils.data import Dataset

class CustomDataset(Dataset):
    def __init__(self,labels,embeddings):
        self.labels=labels
        self.embeddings=embeddings

    def __len__(self):
        return len(self.labels)

    def __getitem__(self,idx):
        return self.embeddings[idx],self.labels[idx]

preq_embeddings={}
with open("/home/jrubiope/FslQnlp/resources/embeddings/common_crawl/glove.42B.300d.txt", 'r', encoding="utf-8") as f:
    for line in f:
        values = line.split()
        word = values[0]
        vector = np.asarray(values[1:], "double")
        preq_embeddings[word] = vector

TESTING=True

words=[]
embeddings=[]
for key, value in preq_embeddings.items():
    words.append(key)
    embeddings.append(value)

if TESTING:
    N=90
    training_words=words[:N]
    training_embeddings=embeddings[:N]

    dev_words=words[N:N+2*N]
    dev_embeddings=embeddings[N:N+2*N]


training_embeddings=np.array([np.reshape(embedding,(1,300)) for embedding in training_embeddings])
dev_embeddings=np.array([np.reshape(embedding,(1,300)) for embedding in dev_embeddings])

def create_data(embeddings):
    training_data=[]
    training_labels=[]
    for i in range(len(embeddings)):
        for j in range(len(embeddings)):
            cos=dot(embeddings[i][0], embeddings[j][0])/(norm(embeddings[i][0])*norm(embeddings[j][0]))
            training_labels.append(cos)
            new_embedding=np.append(embeddings[i],embeddings[j])

            training_data.append(new_embedding)

    training_data=torch.tensor(np.array(training_data),requires_grad=True,device=device)

    training_labels=torch.tensor(np.array(training_labels),requires_grad=True,device=device)
    training_labels_square=torch.square(training_labels)

    return training_data, training_labels_square

y_gate=torch.tensor([[0,-1j],[1j,0]],dtype=torch.complex128,requires_grad=True,device=device)
x_gate=torch.tensor([[0,1],[1,0]],dtype=torch.complex128,requires_grad=True,device=device)
z_gate=torch.tensor([[1,0],[0,-1]],dtype=torch.complex128,requires_grad=True,device=device)

def Id():
    gate=torch.tensor([[1.+0j,0],[0,1.+0j]],requires_grad=True,dtype=torch.complex128,device=device)
    gate.retain_grad()
    return gate

def zero_bra():
    gate=torch.tensor([1.+0j,0,0,0,0,0,0,0],requires_grad=True,dtype=torch.complex128,device=device)
    gate.retain_grad()
    return gate

def zero_ket():
    gate=torch.tensor([[1.+0j],[0],[0],[0],[0],[0],[0],[0]],requires_grad=True,dtype=torch.complex128,device=device)
    gate.retain_grad()
    return gate

def zero_1d_ket():
    gate=torch.tensor([[1.+0j],[0]],requires_grad=True,dtype=torch.complex128,device=device)
    gate.retain_grad()
    return gate

def zero_1d_bra():
    gate=torch.tensor([[1.+0j,0]],requires_grad=True,dtype=torch.complex128,device=device)
    gate.retain_grad()
    return gate

def one_1d_ket():
    gate=torch.tensor([[0],[1.+0j]],requires_grad=True,dtype=torch.complex128,device=device)
    gate.retain_grad()
    return gate

def one_1d_bra():
    gate=torch.tensor([[0,1.+0j]],requires_grad=True,dtype=torch.complex128,device=device)
    gate.retain_grad()
    return gate

def Ry(theta):
    gate = torch.linalg.matrix_exp(-0.5j*theta[:,:,None]*y_gate)
    return gate

def Rx(theta):
    gate = torch.linalg.matrix_exp(-0.5j*theta[:,:,None]*x_gate)
    return gate

def Rx(theta):
    gate = torch.linalg.matrix_exp(-0.5j*theta[:,:,None]*z_gate)
    return gate

CRx=lambda x: kron(Id(),matmul(zero_1d_ket(),zero_1d_bra()))+kron(Rx(x),matmul(one_1d_ket(),one_1d_bra()))

training_data,training_labels=create_data(training_embeddings)
dev_data,dev_labels=create_data(dev_embeddings)

test_data = torch.randn(15,600,requires_grad = True,dtype=torch.double)
test_labels= torch.randn(15,requires_grad = True,dtype=torch.double)

B_SIZE=(1 if N==1 else round(N*N/2))
Total=round(N*N/B_SIZE)

training_object=CustomDataset(training_labels,training_data)
dev_object=CustomDataset(dev_labels,dev_data)
test_object=CustomDataset(test_labels,test_data)

training_loader=DataLoader(training_object,batch_size=B_SIZE)
validation_loader=DataLoader(dev_object,batch_size=B_SIZE)

test_loader = DataLoader(test_object)
validation_test_loader = DataLoader(test_object)

loss_fn = torch.nn.MSELoss()

N_PARAMS=8
class PreQ(nn.Module):
    def __init__(self):
        super(PreQ,self).__init__()
        self.flatten = nn.Flatten(start_dim=0)
        self.linear_relu_stack = nn.Sequential(
            nn.Linear(300, 200),
            nn.ReLU(),
            nn.Linear(200, 100),
            nn.ReLU(),
            nn.Linear(100, 50),
            nn.ReLU(),
            nn.Linear(50,N_PARAMS)
        )
        self.double()

    def forward(self, x):
        #x = self.flatten(x)
        logits1=self.linear_relu_stack(x[:,0:300])
        logits2=self.linear_relu_stack(x[:,300:600])
        # ic(logits1.requires_grad,logits1.is_leaf)
        # ic(logits2.requires_grad,logits2.is_leaf)

        logits1_reshaped=torch.reshape(logits1,(logits1.shape[0],logits1.shape[1],1))
        logits2_reshaped=torch.reshape(logits2,(logits2.shape[0],logits2.shape[1],1))

        # ic(logits1_reshaped.requires_grad,logits1_reshaped.is_leaf)
        # ic(logits2_reshaped.requires_grad,logits2_reshaped.is_leaf)

        bra=torch.stack([zero_bra()[None] for i in range(logits1_reshaped.shape[0])])
        ket=torch.stack([zero_ket() for i in range(logits1_reshaped.shape[0])])

        # ic(bra.requires_grad,bra.is_leaf)
        # ic(ket.requires_grad,bra.is_leaf)

        circuit1=bmm(bra,self.get_quantum_state(parameters=logits2_reshaped).mH)
        circuit2=bmm(self.get_quantum_state(parameters=logits1_reshaped),ket)

        #ic(circuit1.requires_grad,circuit1.is_leaf)
        #ic(circuit2.requires_grad,circuit2.is_leaf)

        inner_product=self.flatten(bmm(circuit1,circuit2))
        fidelity=torch.square(torch.abs(inner_product))

        #output squared is the fidelity of t
        #ic(output.requires_grad,output.is_leaf)


        return fidelity

    def get_quantum_state(self,parameters):
        first_layer=torch.stack( [   kron(kron( Rx(parameters[:,0])[i],Rx(parameters[:,1])[i] ),Rx(parameters[:,2])[i] )  for i in range(parameters.shape[0])   ]   )
        second_layer=torch.stack( [   kron(kron( Ry(parameters[:,0])[i],Ry(parameters[:,1])[i] ),Ry(parameters[:,2])[i] )  for i in range(parameters.shape[0])   ]  )
        third_layer=kron(CRx(parameters[:,6]),Id())
        fourth_layer=kron(Id(),CRx(parameters[:,7]))

        # ic(first_layer.requires_grad,first_layer.is_leaf)
        # ic(second_layer.requires_grad,second_layer.is_leaf)
        # ic(third_layer.requires_grad,third_layer.is_leaf)
        # ic(fourth_layer.requires_grad,fourth_layer.is_leaf)

        output=bmm(bmm(bmm(first_layer,second_layer),third_layer),fourth_layer)
        return output

model = PreQ().to(device)
optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)
optimizer = torch.optim.Adam(model.parameters(), lr=0.006)

def train_one_epoch(epoch_index, tb_writer):
    running_loss = 0.
    last_loss = 0.

    for i, data in enumerate(training_loader):
        # Every data instance is an input + label pair
        inputs, labels = data
        # ic(inputs.shape)


        # Zero your gradients for every batch!
        optimizer.zero_grad()

        # Make predictions for this batch

        #ic(inputs,labels)
        output = model(inputs)
        # ic(output,labels)


        # Compute the loss and its gradients
        loss = loss_fn(output, labels)
        loss.backward(retain_graph = True)
        # ic(loss.item())


        # for name, param in model.named_parameters():
        #     print(name, param.grad)

        # Adjust learning weights
        optimizer.step()

        # Gather data and report
        running_loss += loss.item()
        mm=i%B_SIZE
        # ic(i,mm)
        # if i % B_SIZE == B_SIZE-1:
        if i == Total-1:
            last_loss = running_loss / B_SIZE # loss per batch
            print('  batch {} loss: {}'.format(i + 1, last_loss*100))
            tb_x = epoch_index * len(training_loader) + i + 1
            tb_writer.add_scalar('Loss/train', last_loss, tb_x)
            running_loss = 0.

    return last_loss

# Initializing in a separate cell so we can easily add more epochs to the same run
timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
runs='/home/jrubiope/FslQnlp/runs/NN_outputs/Prueba{}'.format(timestamp)
print(runs)
writer = SummaryWriter(runs)
epoch_number = 0
EPOCHS = 15
best_vloss = 1_000_000.

for epoch in range(EPOCHS):
    print('EPOCH {}:'.format(epoch_number + 1))

    # Make sure gradient tracking is on, and do a pass over the data
    model.train(True)
    avg_loss = train_one_epoch(epoch_number, writer)


    running_vloss = 0.0
    # Set the model to evaluation mode, disabling dropout and using population
    # statistics for batch normalization.
    model.eval()

    # Disable gradient computation and reduce memory consumption.
    with torch.no_grad():
        for i, vdata in enumerate(validation_loader):
            vinputs, vlabels = vdata

            voutputs = model(vinputs)

            vloss = loss_fn(voutputs, vlabels)
            running_vloss += vloss

    avg_vloss = running_vloss / (i + 1)
    print('LOSS-- Train: {} Valid: {}'.format(avg_loss*100, avg_vloss*100))

    # Log the running loss averaged per batch
    # for both training and validation
    writer.add_scalars('Training vs. Validation Loss',
                    { 'Training' : avg_loss, 'Validation' : avg_vloss },
                    epoch_number + 1)
    writer.flush()

    # Track best performance, and save the model's state
    if avg_vloss < best_vloss:
        best_vloss = avg_vloss
        model_path = '/home/jrubiope/FslQnlp/runs/NN/bes_model_{}_{}'.format(timestamp, epoch_number)
        torch.save(model.state_dict(), model_path)

    epoch_number += 1
