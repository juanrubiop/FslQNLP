{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lambeq import BobcatParser, AtomicType, SpacyTokeniser, Rewriter\n",
    "import numpy as np\n",
    "\n",
    "from lambeq import TketModel, QuantumTrainer, SPSAOptimizer,remove_cups\n",
    "\n",
    "#from lambeq import remove_cups\n",
    "\n",
    "from pytket.extensions.qiskit import AerBackend\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from lambeq import AtomicType,BinaryCrossEntropyLoss, Dataset\n",
    "\n",
    "from lambeq import NumpyModel\n",
    "\n",
    "from lambeq import IQPAnsatz,Sim15Ansatz\n",
    "\n",
    "import datetime\n",
    "\n",
    "from utils.FslAnsatz import FslSim15Ansatz, FslStronglyEntanglingAnsatz, FslBaseAnsatz\n",
    "\n",
    "import multiprocessing\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = BobcatParser(verbose='text')\n",
    "tokeniser = SpacyTokeniser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    preq_embeddings={}\n",
    "    with open(\"resources\\embeddings\\common_crawl\\glove.42B.300d.txt\", 'r', encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], \"float32\")\n",
    "            preq_embeddings[word] = vector\n",
    "    return preq_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filename):\n",
    "    labels, sentences = [], []\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            t = int(line[0])\n",
    "            labels.append([t, 1-t])\n",
    "            sentences.append(line[1:].strip())\n",
    "    return labels, sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_diagrams(train_data,dev_data,test_data,OOV_test_data,redundant_test_data):\n",
    "    raw_train_tokens = tokeniser.tokenise_sentences(train_data)\n",
    "    raw_train_tokens = [tokens[:-1] for tokens in raw_train_tokens]\n",
    "\n",
    "    raw_dev_tokens = tokeniser.tokenise_sentences(dev_data)\n",
    "    raw_dev_tokens = [tokens[:-1] for tokens in raw_dev_tokens]\n",
    "\n",
    "\n",
    "    raw_test_tokens = tokeniser.tokenise_sentences(test_data)\n",
    "    raw_test_tokens =  [tokens[:-1] for tokens in raw_test_tokens]\n",
    "\n",
    "    raw_OOV_test_tokens = tokeniser.tokenise_sentences(OOV_test_data)\n",
    "    raw_OOV_test_tokens = [tokens[:-1] for tokens in raw_OOV_test_tokens]\n",
    "\n",
    "    raw_redundancy_test_tokens = tokeniser.tokenise_sentences(redundant_test_data)\n",
    "    raw_redundancy_test_tokens = [tokens[:-1] for tokens in raw_redundancy_test_tokens]\n",
    "\n",
    "    train_diagrams = parser.sentences2diagrams(raw_train_tokens,tokenised=True)\n",
    "    dev_diagrams = parser.sentences2diagrams(raw_dev_tokens,tokenised=True)\n",
    "    test_diagrams = parser.sentences2diagrams(raw_test_tokens,tokenised=True)\n",
    "    OOV_test_diagrams = parser.sentences2diagrams(raw_OOV_test_tokens,tokenised=True)\n",
    "    redundancy_test_diagrams = parser.sentences2diagrams(raw_redundancy_test_tokens,tokenised=True)\n",
    "\n",
    "    #train_diagrams = [remove_cups(diagram) for diagram in train_diagrams]\n",
    "    #dev_diagrams = [remove_cups(diagram) for diagram in dev_diagrams]\n",
    "    #test_diagrams = [remove_cups(diagram) for diagram in test_diagrams]\n",
    "    #OOV_test_diagrams = [remove_cups(diagram) for diagram in OOV_test_diagrams]\n",
    "    #redundancy_test_diagrams = [remove_cups(diagram) for diagram in redundancy_test_diagrams]\n",
    "    \n",
    "    return train_diagrams, dev_diagrams, test_diagrams,OOV_test_diagrams,redundancy_test_diagrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels, train_data = read_data('resources/dataset/new_mc_train_data.txt')\n",
    "dev_labels, dev_data = read_data('resources/dataset/new_mc_dev_data.txt')\n",
    "test_labels, test_data = read_data('resources/dataset/new_mc_test_data_seen.txt')\n",
    "OOV_test_labels, OOV_test_data = read_data('resources/dataset/new_mc_test_data_OOV.txt')\n",
    "redundant_test_labels, redundant_test_data = read_data('resources/dataset/new_mc_test_data_redundancy.txt')\n",
    "\n",
    "TESTING=True\n",
    "\n",
    "if TESTING:\n",
    "    train_labels, train_data = train_labels[:2], train_data[:2]\n",
    "    dev_labels, dev_data = dev_labels[:2], dev_data[:2]\n",
    "    test_labels, test_data = test_labels[:2], test_data[:2]\n",
    "    OOV_test_labels, OOV_test_data = OOV_test_labels[:2], OOV_test_data[:2]\n",
    "    redundant_test_labels, redundant_test_data = redundant_test_labels[:2], redundant_test_data[:2]\n",
    "    EPOCHS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tagging sentences.\n",
      "Parsing tagged sentences.\n",
      "Turning parse trees to diagrams.\n",
      "Tagging sentences.\n",
      "Parsing tagged sentences.\n",
      "Turning parse trees to diagrams.\n",
      "Tagging sentences.\n",
      "Parsing tagged sentences.\n",
      "Turning parse trees to diagrams.\n",
      "Tagging sentences.\n",
      "Parsing tagged sentences.\n",
      "Turning parse trees to diagrams.\n",
      "Tagging sentences.\n",
      "Parsing tagged sentences.\n",
      "Turning parse trees to diagrams.\n"
     ]
    }
   ],
   "source": [
    "train_diagrams, dev_diagrams, test_diagrams,OOV_test_diagrams,redundancy_test_diagrams=generate_diagrams(train_data=train_data,dev_data=dev_data,test_data=test_data,OOV_test_data=OOV_test_data,redundant_test_data=redundant_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_circuits(map,n_layers,ansatz_string,preq_embeddings):\n",
    "    match ansatz_string:\n",
    "        case \"FslBase\":\n",
    "            ansatz = FslBaseAnsatz(preq_embeddings,map, n_layers=n_layers)\n",
    "        case \"FslSim15\":\n",
    "            ansatz = FslSim15Ansatz(preq_embeddings,map, n_layers=n_layers)  \n",
    "        case \"Sim15Ansatz\":\n",
    "            ansatz = Sim15Ansatz(map,n_layers=n_layers, n_single_qubit_params=3)\n",
    "\n",
    "    train_circuits = [ansatz(diagram) for diagram in train_diagrams]\n",
    "    print(\"Train circuits done\")\n",
    "    dev_circuits =  [ansatz(diagram) for diagram in dev_diagrams]\n",
    "    print(\"Dev circuits done\")\n",
    "    test_circuits = [ansatz(diagram) for diagram in test_diagrams]\n",
    "    print(\"Test circuits done\")\n",
    "    OOV_test_circuits = [ansatz(diagram) for diagram in OOV_test_diagrams]\n",
    "    print(\"OOV circuits done\")\n",
    "    redundancy_test_circuits = [ansatz(diagram) for diagram in redundancy_test_diagrams]\n",
    "    print(\"Redundant circuits done\")\n",
    "\n",
    "    return train_circuits, dev_circuits, test_circuits, OOV_test_circuits, redundancy_test_circuits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_model(model_string,all_circuits,checkpoint,logdir=''):\n",
    "    match model_string:\n",
    "        case \"Numpy\":\n",
    "            if checkpoint:\n",
    "                    model = NumpyModel.from_checkpoint(logdir+'\\model.lt')\n",
    "            else:\n",
    "                    model = NumpyModel.from_diagrams(all_circuits, use_jit=True)\n",
    "        case \"Tket\":\n",
    "            backend = AerBackend()\n",
    "            backend_config = {\n",
    "                'backend': backend,\n",
    "                'compilation': backend.default_compilation_pass(2),\n",
    "                'shots': 8192\n",
    "            }\n",
    "            model = TketModel.from_diagrams(all_circuits, backend_config=backend_config)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_everything(logdir,loss_function,acc_function,a,c,A,model,trainer,test_acc):\n",
    "    print(\"Saving everything\")\n",
    "    acc = lambda y_hat, y: np.sum(np.round(y_hat) == y) / len(y) / 2  # half due to double-counting\n",
    "\n",
    "    fig, ((ax_tl, ax_tr), (ax_bl, ax_br)) = plt.subplots(2, 2, sharex=True, sharey='row', figsize=(10, 6))\n",
    "    ax_tl.set_title('Training set')\n",
    "    ax_tr.set_title('Development set')\n",
    "    ax_bl.set_xlabel('Iterations')\n",
    "    ax_br.set_xlabel('Iterations')\n",
    "    ax_bl.set_ylabel('Accuracy')\n",
    "    ax_tl.set_ylabel('Loss')\n",
    "\n",
    "    colours = iter(plt.rcParams['axes.prop_cycle'].by_key()['color'])\n",
    "    range_ = np.arange(1, trainer.epochs + 1)\n",
    "    ax_tl.plot(range_, trainer.train_epoch_costs, color=next(colours))\n",
    "    ax_bl.plot(range_, trainer.train_eval_results['acc'], color=next(colours))\n",
    "    ax_tr.plot(range_, trainer.val_costs, color=next(colours))\n",
    "    ax_br.plot(range_, trainer.val_eval_results['acc'], color=next(colours))\n",
    "    plt.savefig(logdir+'\\plot.png')\n",
    "\n",
    "\n",
    "    best_model=NumpyModel.from_checkpoint(logdir+'\\\\best_model.lt')\n",
    "    best_model_test_acc = acc(best_model(test_circuits), test_labels)\n",
    "    model=NumpyModel.from_checkpoint(logdir+'\\\\model.lt')\n",
    "    test_acc = acc(model(test_circuits), test_labels)\n",
    "\n",
    "    bm_OOV_test_acc= acc(best_model(OOV_test_circuits), OOV_test_labels)\n",
    "    OOV_test_acc= acc(model(OOV_test_circuits), OOV_test_labels)\n",
    "\n",
    "    bm_redundant_test_acc= acc(best_model(redundancy_test_circuits), redundant_test_labels)\n",
    "    redundant_test_acc= acc(model(redundancy_test_circuits), redundant_test_labels)\n",
    "\n",
    "    file_path = f\"{logdir}/info_file.txt\"\n",
    "    with open(file_path, 'w') as file:\n",
    "        # Write the input string to the file\n",
    "        input_string=f\"\"\"Task: Meaning classification\n",
    "    Classical Embeddings: GloVe 50-d\n",
    "    Parsing: True\n",
    "    Rewritign: Remove Cups\n",
    "    Ansatz: {ansatz_string}\n",
    "    Layers: {n_layers}\n",
    "    Map: [N:{map[N]}, S:{map[S]}]\n",
    "    Model: Numpy\n",
    "    Backend: None\n",
    "    Trainer: Quantum Trainer\n",
    "    Loss function: {loss_function}\n",
    "    Accuracy function: {acc_function}\n",
    "    Optimizer: SPSA optimizer\n",
    "    Epochs: {EPOCHS}\n",
    "    Batch size: {BATCH_SIZE}\n",
    "    Seed: {SEED}\n",
    "    Hyperparams: [a:{a},c:{c},A:{A}]\n",
    "    Test accuracy: {test_acc}\n",
    "    Test accuracy best model: {best_model_test_acc}\n",
    "    OOV test accuracy: {OOV_test_acc}\n",
    "    OOV test accuracy best model: {bm_OOV_test_acc}\n",
    "    Redundant test accuarcy: {redundant_test_acc}\n",
    "    Redundant test accuracy best model: {bm_redundant_test_acc}\"\"\"\n",
    "        file.write(input_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_run(EPOCHS, SEED, BATCH_SIZE,MODEL):\n",
    "    # Using the builtin binary cross-entropy error from lambeq\n",
    "    acc = lambda y_hat, y: np.sum(np.round(y_hat) == y) / len(y) / 2  # half due to double-counting\n",
    "    bce = BinaryCrossEntropyLoss()\n",
    "    loss_function=\"BindaryCrosEntropyLoss\"\n",
    "    acc_function=\"lambda y_hat, y: np.sum(np.round(y_hat) == y) / len(y) / 2\"\n",
    "\n",
    "    a=0.05\n",
    "    c=0.06\n",
    "    A=\"0.1*Epochs\"\n",
    "    logdir='runs\\Proper\\Epochs_{}--A_{}--N_{}--S_{}--L_{}--Ansatz_{}\\Seed_{}'.format(EPOCHS,a,map[N],map[S],n_layers,ansatz_string,SEED)\n",
    "    print('Initialize trainer')\n",
    "    trainer = QuantumTrainer(\n",
    "        model=MODEL,\n",
    "        loss_function=bce,\n",
    "        epochs=EPOCHS,\n",
    "        optimizer=SPSAOptimizer,\n",
    "        optim_hyperparams={'a': a, 'c': 0.06, 'A':0.01*EPOCHS},\n",
    "        evaluate_functions={'acc': acc},\n",
    "        evaluate_on_train=True,\n",
    "        verbose = 'text',\n",
    "        seed=SEED,\n",
    "        from_checkpoint=checkpoint,\n",
    "        log_dir=logdir\n",
    "    )\n",
    "\n",
    "    train_dataset = Dataset(\n",
    "                train_circuits,\n",
    "                train_labels,\n",
    "                batch_size=BATCH_SIZE)\n",
    "\n",
    "    val_dataset = Dataset(dev_circuits, dev_labels, shuffle=False)\n",
    "\n",
    "    now = datetime.datetime.now()\n",
    "    t = now.strftime(\"%Y-%m-%d_%H_%M_%S\")\n",
    "    print(t)\n",
    "    print('Starting fit')\n",
    "    trainer.fit(train_dataset, val_dataset, log_interval=10)\n",
    "    test_acc = 'acc(model(test_circuits), test_labels)'\n",
    "\n",
    "    save_everything(logdir=logdir,loss_function=loss_function,acc_function=acc_function,a=a,c=c,A=A,model=MODEL,trainer=trainer,test_acc=test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    " preq_embeddings=load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_everything(N_qubit):\n",
    "\n",
    "    # Define atomic types\n",
    "    N = AtomicType.NOUN\n",
    "    S = AtomicType.SENTENCE\n",
    "\n",
    "    n_layers=1\n",
    "\n",
    "    print(\"Worker process id for {0}: {1}\".format(N_qubit, os.getpid()))\n",
    "\n",
    "    alpha=\"Sim15Ansatz\"\n",
    "    beta=\"FslBase\"\n",
    "    gamma=\"FslSim15\"\n",
    "    ansatz_string=alpha\n",
    "    \n",
    "    map={N:N_qubit,S:1}\n",
    "\n",
    "\n",
    "    train_circuits, dev_circuits, test_circuits,OOV_test_circuits, redundancy_test_circuits=create_circuits(map=map,n_layers=n_layers,ansatz_string=ansatz_string,preq_embeddings=preq_embeddings)\n",
    "    print(\"Circuit Processing finished\")\n",
    "\n",
    "    all_circuits = train_circuits+dev_circuits+test_circuits+OOV_test_circuits+redundancy_test_circuits\n",
    "\n",
    "    return all_circuits\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Turning sentences to circuits\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    total_N=[1,2,3,4,5]\n",
    "\n",
    "    print(\"Turning sentences to circuits\")\n",
    "\n",
    "    p = multiprocessing.Pool()\n",
    "\n",
    "    all_circuits = p.map(prepare_everything, total_N) \n",
    "\n",
    "    all_circuits=prepare_everything(N_qubit=N_qubit,preq_embeddings=preq_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    checkpoint=False\n",
    "\n",
    "    print(\"Setting model\")\n",
    "    \n",
    "    model=set_model(model_string=\"Numpy\",checkpoint=checkpoint,all_circuits=all_circuits)\n",
    "\n",
    "    seed_arr = [0, 10, 50, 77, 100, 111, 150, 169, 200, 234, 250, 300, 350, 400, 450]\n",
    "    #seed_arr = [10, 50, 77, 100, 111, 150, 169, 200, 234, 250, 300, 350, 400, 450]\n",
    "\n",
    "    B_sizes = [700]\n",
    "    epochs_arr = [2000]\n",
    "\n",
    "    for SEED in seed_arr:\n",
    "        for BATCH_SIZE in B_sizes:\n",
    "            for EPOCHS in epochs_arr:\n",
    "                print(EPOCHS, SEED, BATCH_SIZE)\n",
    "                main_run(EPOCHS, SEED, BATCH_SIZE,MODEL=model)\n",
    "\n",
    "    now = datetime.datetime.now()\n",
    "    t = now.strftime(\"%Y-%m-%d_%H_%M_%S\")\n",
    "    print(t)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
